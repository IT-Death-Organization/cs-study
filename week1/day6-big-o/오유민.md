# Day 6: 시간복잡도와 공간복잡도 (Big-O)

> 📅 2025.02.18 | 📁 Week 1

---

## 🧠 학습 질문

- [x] Big-O, Big-Θ, Big-Ω의 차이는 무엇이며 각각 언제 사용하는가?
- [x] O(n)과 O(2n)이 같은 이유는? 상수를 무시하는 이유는?
- [x] O(n²)과 O(n² + n)이 같은 이유는? 하위 차수를 무시하는 이유는?
- [x] 최선/평균/최악의 경우를 각각 언제 고려하는가?
- [x] Amortized 시간복잡도란 무엇이며 일반 시간복잡도와 어떻게 다른가?
- [x] 시간복잡도와 공간복잡도를 트레이드오프 해야 할 때 어떻게 판단하는가?

---

### 📌 [Q1] Big-O, Big-Θ, Big-Ω의 차이는 무엇이며 각각 언제 사용하는가?

**Big-O (빅오) 표기법: 상한선 (Upper Bound)**

- 알고리즘의 실행 시간이 아무리 최악이라도 이 선은 넘지 않는다는 **최악의 경우** 나타냄 (최고차항)
- f(n)≤c⋅g(n) 을 만족하면 → O(g(n))
- 보통 실무에서 가장 많이 사용 (정렬 알고리즘 성능 비교, 서버 부하 예측, 면접 등)

**Big-$\Omega$ (빅 오메가) 표기법: 하한선 (Lower Bound)**

- 알고리즘이 아무리 빨라도 이 시간보다는 더 걸린다는 **최선의 경우** 나타냄, 알고리즘의 최소 수행 시간 보장
- f(n)≥c⋅g(n) 이면 → Ω(g(n))
- 최선의 경우를 분석할 때, 어떤 문제의 이론적 최소 한계를 말할 때 사용 (ex. 비교 기반 정렬은 Ω(n log n) 이하로 못 내려간다.)

**Big-$\Theta$ (빅 세타) 표기법: 딱 맞는 선 (Tight Bount)**

- 상한과 하한이 일치할 때 사용, 알고리즘의 **평균적인 성능**을 가장 정확하게 묘사
- c1g(n) ≤ f(n) ≤ c2g(n) → Θ(g(n))
- 정확한 분석이 필요할 때 사용

ex) 숫자 10개가 담긴 배열에서 특정 값을 찾는 상황

- 빅오 표기법 (상한선): 마지막에 있거나 배열에 없음 -> O(N)
- 빅 오메가 표기법 (하한선): 첫 번째에서 바로 찾음 -> O(1)
- 빅 세타 표기법 (평균): 보통 중간 어디쯤에 있음 -> O(N)

**정리** <br>
Big-O는 알고리즘의 최악의 경우 시간 복잡도의 상한을 나타내며,
Big-Ω는 최소 수행 시간의 하한을 의미합니다.
Big-Θ는 상한과 하한이 일치하는 경우로, 정확한 성장률을 나타냅니다.
실무에서는 보통 최악의 경우를 고려하기 때문에 Big-O를 가장 많이 사용합니다.

### 📌 [Q2] O(n)과 O(2n)이 같은 이유는? 상수를 무시하는 이유는?

빅오 표기법의 목적이 **"입력 크기에 따른 증가율"**을 파악하는 데 있기 때문이다.
점근적 분석의 관점에서 이유를 정리할 수 있다.

```
f(n) = 2n
```

빅오 정의에 따르면 `f(n) ≤ c · g(n)`을 만족하면 O(g(n))이다.
위의 예시에서 `f(n) = 2n`, `g(n) = n`이라면 **`2n <= 2 * n`** 이므로 `c=2`로 성립한다.
즉, `2n ∈ O(n)`이므로 `O(2n) = O(n)`이 된다.

빅오 표기법에서 상수를 무시하는 이유는 **증가율(성장 속도)만 중요**하기 때문이다.
빅오는 **n이 무한히 커질 때** 어떻게 증가하는지를 본다.

$n$이 1,000만 배가 되면 $2n$도 결국 2,000만 배가 된다. 즉, 실행 시간이 입력값에 비례해서 직선적으로 늘어난다는 본질적인 특성은 변하지 않는 것이다.
반면 $O(n)$과 $O(n^2)$은 차원이 다르다. $n$이 100만 배 커질 때, $n^2$은 1조 배 커진다. 이 정도의 격차에 비하면 앞에 붙은 상수 '2'는 무시해도 될 만큼 미미한 차이가 된다.

**정리** <br>
O(n)과 O(2n)이 같은 이유는 Big-O가 함수의 성장률을 나타내는 표기법이기 때문입니다.
상수배 차이는 입력이 충분히 커지면 증가 속도에 영향을 주지 않으므로 무시합니다.
Big-O는 상한을 나타내며, 상수는 정의상 항상 어떤 c로 흡수될 수 있습니다.

### 📌 [Q3] O(n²)과 O(n² + n)이 같은 이유는? 하위 차수를 무시하는 이유는?

입력이 충분히 커지면, 가장 큰 차수(term)가 전체 성능을 지배하기 때문이다.
n이 커질수록 n²은 n보다 훨씬 빠르게 증가하므로 전체 값은 n²에 의해 결정된다.
`n² + n ∈ O(n²)`

하위 차수를 무시하는 이유는, 빅오 표기법은 **성장률 비교가 목적이기 때문**이다.
빅오는 **입력이 무한히 커질 때 증가 속도는?**을 보는 도구이다.

**정리**<br>
O(n²)과 O(n² + n)이 같은 이유는 입력이 충분히 커지면 n² 항이 n 항보다 훨씬 빠르게 증가하여 전체 성능을 지배하기 때문입니다.
Big-O는 점근적(asymptotic) 성장률을 나타내므로 상수와 하위 차수는 무시하고 최고차항만 고려합니다.

### 📌 [Q4] 최선/평균/최악의 경우를 각각 언제 고려하는가?

**최악의 경우 - 빅오 표기법**

- 주로 실무, 시스템의 안정성과 신뢰성이 중요한 시스템
- 이 알고리즘은 절대 이 이상 느려지지 않습니다라는 성능 보장이 필요할 때
- ex) 실시간 시스템, 금융 시스템, 서버 트레픽 처리
- ex) 선형 탐색 -> O(N), 병합 정렬 -> 항상 O(n log n)

**평균인 경우 - 빅 세타 표기법**

- 실제 서비스 성능이 중요할 때, 대부분의 입력이 랜덤일 때, 체감 속도가 중요할 때
- ex) 퀵 정렬 -> 평균: O(n log n), 최악: O(n²)

**최선의 경우 - 빅 오메가 표기법**

- 알고리즘의 잠재적 최소 성능을 볼 때, 특정 조건에서 매우 빠른지 볼 때, 이론적 하한을 설명할 때
- ex) 선형 탐색 -> 첫 번째에서 찾으면 O(1)

### 📌 [Q5] Amortized 시간복잡도란 무엇이며 일반 시간복잡도와 어떻게 다른가?

Amortized(분할 상환) 시간복잡도란, 연산을 여러 번 수행했을 때 **연산 1회당 평균 비용을 계산하는 방식**을 말한다. 즉, 전체 연산을 기준으로 어떤 연산은 가끔 매우 비싸지만 전체적으로 보면 평균적으로는 싼 경우이다. 반면 일반 시간복잡도는 **1번의 연산이 최악의 경우**에 얼마나 걸리는지를 본다.

**EX) 동적 배열 (dynamic array)**
C++의 vector나 Java의 ArrayList를 통해 구현할 수 있는 동적 배열이 그 예시이다.
배열이 꽉 찼을 때 동적 배열은 **"새 배열 생성 & 기존 요소 전부 복사"**를 하므로 `O(N)`이 걸린다.

전체 과정을 보면, 이 동적 배열의 확장 과정은 (1 → 2 → 4 → 8 → 16 → 32 ...)이다.
총 복사 비용을 계산하면 `1 + 2 + 4 + 8 + ... + n = 2n - 1 = O(n)`이다.
그런데 n번의 append를 한 것이므로 O(N)/n = O(1)이다. 따라서 append의 amortized 시간복잡도는 O(1)이 된다.

**Amortized 시간복잡도 != 평균 시간복잡도**

- Amortized 시간복잡도
  - 어떤 입력이 오더라도 보장되는 평균 비용
  - 최악의 시나리오 포함
  - 입력 가정 없음 (데이터가 어떻게 들어오든 **연산을 반복하기만 하면 반드시 수렴하는 성능**)
- 평균 시간복잡도
  - 확률 기반 평균 (데이터가 랜덤하게 들어올 것이라는 확률에 기반)
  - 입력 분포 가정

**정리**<br>
Amortized 시간복잡도는 여러 번의 연산을 전체적으로 분석하여 연산 1회당 평균 비용을 계산하는 방식입니다.
특정 연산은 최악의 경우 O(n)이 걸릴 수 있지만, 전체 수행 횟수를 기준으로 나누면 평균적으로는 O(1)이 될 수 있습니다.
이는 확률 기반 평균과는 다르며, 어떤 입력에 대해서도 보장되는 평균 비용이라는 점이 특징입니다.

### 📌 [Q6] 시간복잡도와 공간복잡도를 트레이드오프 해야 할 때 어떻게 판단하는가?

**시간을 우선해야 하는 경우 (속도 > 메모리)**

- 실시간 응답이 중요한 서비스
- 메모리 자원이 비교적 여유로운 환경
- 반복 계산이 많은 경우 (같은 연산을 반복해야 한다면, 결과를 메모리에 저장(캐싱)하는 게 무조건 이득)
- 방법론: 메모이제이션(Memoization), 해시 테이블(Hash Table) 사용, 미리 계산된 테이블(Look-up Table) 활용

**공간을 우선해야 하는 경우 (메모리 > 속도)**

- 임베디드/IoT 환경 (가용 메모리가 kb 단위인 하드웨어 제어 시스템)
- 대규모 데이터 처리 (데이터가 너무 커서 메모리에 다 올릴 수 없을 때)
- 모바일 성능/배터리 최적화 (과도한 메모리 점유는 GC를 유발해 오히려 시스템 전체 느리게 만듦)
- 방법론: 인플레이스(In-place) 알고리즘 사용, 반복문 대신 재귀를 피하거나(스택 오버플로우 방지), 스트리밍 방식 처리

**트레이드오프 사례**

1. 메모이제이션 (DP)
   상황: 피보나치 수열을 재귀로 풀면 $O(2^n)$이지만, 계산 결과를 배열에 저장하면 $O(n)$으로 줄어듭니다.
   판단: 배열 몇 개를 만드는 메모리 비용보다, 기하급수적으로 늘어나는 실행 시간을 줄이는 이득이 압도적으로 큽니다. (시간 우선)

2. 해시 테이블 vs 선형 탐색
   상황: 배열에서 특정 요소를 찾으려면 $O(n)$이 걸리지만, 이를 해시 맵으로 바꾸면 $O(1)$에 찾을 수 있습니다. 대신 해시 맵은 추가적인 메모리 공간을 차지합니다.
   판단: 검색이 빈번하게 일어난다면 메모리를 더 써서라도 해시를 쓰는 것이 정답입니다.

3. 정렬 알고리즘 선택
   상황: 퀵 정렬(Quick Sort)은 빠르지만 재귀 스택 공간을 사용합니다. 반면 힙 정렬(Heap Sort)은 추가 공간을 거의 쓰지 않지만 실제 속도는 퀵 정렬보다 약간 느릴 수 있습니다.
   판단: 메모리가 극도로 제한된 환경이라면 힙 정렬을, 일반적인 상황이라면 퀵 정렬을 선택합니다.

**정리**<br>
시간과 공간의 트레이드오프는 시스템 환경과 요구사항에 따라 판단합니다.
실시간 처리나 사용자 응답이 중요한 경우에는 시간을 우선 고려하고,
메모리 제약이 큰 환경에서는 공간 최적화를 우선합니다.
또한 데이터 크기와 확장성을 함께 고려하여 균형 있는 선택을 합니다.

---

## 📎 참고 자료

<!-- 공부하면서 참고한 링크를 여기에 추가해주세요 -->

---

## 💬 토론 포인트

<!-- PR 리뷰 또는 스터디 중 나온 추가 질문이나 논의 사항을 기록해주세요 -->
